{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "from collections import defaultdict \n",
    "import pickle\n",
    "import statistics\n",
    "import scipy.stats as st\n",
    "import pyspark.sql.functions as ps\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.types import *\n",
    "from scipy import stats\n",
    "from shutil import copyfile\n",
    "import pickle as pkl\n",
    "#import databricks.koalas as ks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instructions \n",
    "\n",
    "**NOTE: DATES ARE NOT INCLUSIVE!!! So if you want conversations from November 2019, December 2019, and January 2019 your start period should be 2019-11-01 and your end date should be 2020-02-01 and NOT 2020-01-01**\n",
    "\n",
    "**NOTE: PLEASE CLONE THIS NOTEBOOK INTO YOUR OWN FOLDER AND ATTACH TO DS DEV1 CLUSTER BEFORE RUNNING**\n",
    "\n",
    "1. You must have done a sentiment curation for the code to work properly - please input the output id for which you ran a sentiment curation in the **01. Sent Curation Output ID** widget\n",
    "\n",
    "2. Specify working Amazon s3 working directory for your file to be saved to in the **02. Working Directory** widget - for example /bsinsights/Raim \n",
    "\n",
    "3. Specify a file name in the **03. File Name** widget\n",
    "\n",
    "4. Specify a start date for the \"previous/baseline period\" in the **04. Prev Period Start Date** widget - please note that it MUST be in YYYY-MM-DD format \n",
    "\n",
    "5. Specify an end date for the \"previous/baseline period\" in the **05. Prev Period End Date** widget - please note that it MUST be in YYYY-MM-DD format \n",
    "\n",
    "6. Specify a start date for the \"recent/comparison period\" in the **06. Recent Period Start Date** widget - please note that it MUST be in YYYY-MM-DD format \n",
    "\n",
    "7. Specify an end date for the \"recent/comparison period\" in the **07. Recent Period End Date** widget - please note that it MUST be in YYYY-MM-DD format \n",
    "\n",
    "8. Select which level you want the volumes, sentiment breakdown, and rankings for in the **08. Level** widget. You can select all three levels if you like - just note that the more levels you select, the longer the notebook takes to run.\n",
    " \n",
    "9. Specify the Standard Aviation Category in the **09. Std Aviation Category** widget. This is the category where you have things like \"airports\"/\"countries\" etc. it usually is NOT INCLUDED in when doing sentiment breakdowns and rankings and will add *SIGNIFICANT computation time if not excluded*!!!! \n",
    "\n",
    "10. Specifiy the name of the lense in which the comparison entities are located in the **10. Comp Entity Lense** widget. For example, if you want to compare airline performance against other taxonomy elements you should have a lense named \"Active Airlines\" and within this lense, the sublenses will be the actual airlines which you want to compare.\n",
    "\n",
    "11. Specify the name of the comparison entities in the **11. Comp Entity Name** widget. For example, if want to compare airlines to each other you should put the comparison entity name as \"airline\", if you wanted to compare chip brands to each other you should put the comparison entity as \"Chip Brand\".\n",
    "\n",
    "12. Specify if you want to save your results as an excel file or if just want to use the notebook to visualize/sort your results in the **12. Write to Excel** widget.\n",
    "\n",
    "10. Run all cells - the notebook will take between *15-30 minutes to finish* (depending on how many levels have been choosen) and you will find your excel file with 3 * the number of levels you chose in the working directory that you specified under the file name that you input. For each level there will be 3 tabs in the excel file:\n",
    "\n",
    "  (a) *level_name_df_prev*: the sentiment volumes, percentages, and rankings over the previous/baseline period\n",
    "  \n",
    "  (b) *level_name_df_rec*: the sentiment volumes, percentages, and rankings over the recent/comparison period\n",
    "  \n",
    "  (c) *level_name_df_rankings*: the net sentiment and net sentiment ranks over both the previous/baseline period and the recent/comparison period as well as the net sentiment change between the two periods and the net sentiment rank change between the two periods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Widget Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dbutils.widgets.removeAll()\n",
    "\n",
    "dbutils.widgets.text(\"01. Sent Curation Output ID\", \"\")\n",
    "dbutils.widgets.text(\"02. Working Directory\", \"\")\n",
    "dbutils.widgets.text(\"03. File Name\",\"\")\n",
    "dbutils.widgets.text(\"04. Prev Period Start Date\",\"\")\n",
    "dbutils.widgets.text(\"05. Prev Period End Date\",\"\")\n",
    "dbutils.widgets.text(\"06. Recent Period Start Date\",\"\")\n",
    "dbutils.widgets.text(\"07. Recent Period End Date\",\"\")\n",
    "dbutils.widgets.multiselect(\"08. Level\",\"Category\", [\"Category\",\"Lense\",\"Sublense\"])\n",
    "dbutils.widgets.text(\"09. Excluded Category\",\"\")\n",
    "dbutils.widgets.text(\"10. Comp Entity Lense\",\"\")\n",
    "dbutils.widgets.text(\"11. Comp Entity Name\",\"\")\n",
    "dbutils.widgets.dropdown(\"12. Write to Excel?\",\"No\", ['Yes','No'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = spark.read.table(\"sentiment_curations.\" + str(dbutils.widgets.get(\"01. Sent Curation Output ID\")))\n",
    "p_dates = (str(dbutils.widgets.get(\"04. Prev Period Start Date\")), str(dbutils.widgets.get(\"05. Prev Period End Date\")))\n",
    "r_dates = (str(dbutils.widgets.get(\"06. Recent Period Start Date\")), str(dbutils.widgets.get(\"07. Recent Period End Date\")))\n",
    "output_path = str(dbutils.widgets.get(\"02. Working Directory\"))\n",
    "output_filename = str(dbutils.widgets.get(\"03. File Name\"))\n",
    "levels = dbutils.widgets.get(\"08. Level\").split(\",\")\n",
    "excluded_cat = str(dbutils.widgets.get(\"09. Excluded Category\"))\n",
    "comp_ent_lense = str(dbutils.widgets.get(\"10. Comp Entity Lense\"))\n",
    "comp_ent_name = str(dbutils.widgets.get(\"11. Comp Entity Name\"))\n",
    "to_excel = str(dbutils.widgets.get(\"12. Write to Excel?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# ----------------------------------------- sentiment aggregation and calculation function utils --------------------------------- #\n",
    "'''\n",
    "    agg_cat(entity: str, p:str, input_dict: dict) -> spark dataframe\n",
    "        - calculates the total, positive, and negative volumes and percentages for each comparison entity within each category \n",
    "        - ranks the comparison entities against their performance in each category for positive, negative, and net sentiment\n",
    "\n",
    "    parameters: entity - the name of the column which has the entities you want to compare \n",
    "                    - inhereted from main_agg\n",
    "                p - the period in which you aggregating and calculating volumes, percentages, and rankings for the comparison entities \n",
    "                    - inherted from main_agg \n",
    "                    - takes values \"df_prev\" or \"df_rec\" \n",
    "                input_dict - the dictionary for \"Category\" formatted as: \n",
    "                            input_dict = {'df_initial':spark_df, 'df_prev': spark_df, 'df_rec':spark_df, 'df_rankings': None}\n",
    "                            - inherted from main_agg\n",
    "\n",
    "    returns: a spark dataframe with the sentiment volumes, percentages, and rankings calculated at the category level for all\n",
    "            comparison entities for a given time period\n",
    "'''\n",
    "def agg_cat(entity: str, p: str, input_dict: dict):\n",
    "    df = input_dict[p]\n",
    "    grouping = ['Category',entity]\n",
    "    \n",
    "    df_total = df.groupby(grouping).agg(ps.countDistinct('id').alias('total_vol'))\n",
    "  \n",
    "    df_pos = df.filter(ps.col('sentiment') == 1).groupby(grouping)\\\n",
    "              .agg(ps.countDistinct('id').alias('pos_vol')).withColumnRenamed('Category','Category_pos').withColumnRenamed(entity,entity+'_pos')\n",
    "\n",
    "    df_neg = df.filter(ps.col('sentiment') == -1).groupby(grouping)\\\n",
    "              .agg(ps.countDistinct('id').alias('neg_vol')).withColumnRenamed('Category','Category_neg').withColumnRenamed(entity,entity+'_neg')\n",
    "\n",
    "    df1 = df_total.join(df_pos, on = [df_total['Category'] == df_pos['Category_pos'],\n",
    "                                           df_total[entity] == df_pos[entity+'_pos']],\n",
    "                                      how = 'inner')\n",
    "\n",
    "    df_new = df1.join(df_neg, \n",
    "                         on = [df1['Category'] == df_neg['Category_neg'],\n",
    "                               df1[entity] == df_neg[entity+'_neg']],\n",
    "                                      how = 'inner')\\\n",
    "                        .drop('Category_pos')\\\n",
    "                        .drop(entity+'_pos')\\\n",
    "                        .drop('Category_neg')\\\n",
    "                        .drop(entity+'_neg')\n",
    "    \n",
    "    df_new = df_new.withColumn('percent_pos',  ps.round((ps.col('pos_vol')/ps.col('total_vol'))*100,2))\\\n",
    "            .withColumn('percent_neg', ps.round((ps.col('neg_vol')/ps.col('total_vol'))*100,2))\\\n",
    "            .withColumn('net_sent', ps.round((ps.col('percent_pos')-ps.col('percent_neg')),2))\n",
    "    \n",
    "    wPos = Window.partitionBy('Category').orderBy(ps.desc(\"percent_pos\"))\n",
    "    wNeg = Window.partitionBy('Category').orderBy(ps.desc(\"percent_neg\"))\n",
    "    wNet = Window.partitionBy('Category').orderBy(ps.desc(\"net_sent\"))\n",
    "    df_new = df_new.withColumn(\n",
    "        \"percent_pos_rank\", \n",
    "        ps.dense_rank().over(wPos)\n",
    "    ).withColumn(\n",
    "        \"percent_neg_rank\", \n",
    "        ps.dense_rank().over(wNeg)\n",
    "    ).withColumn(\n",
    "        \"net_sent_rank\",\n",
    "        ps.dense_rank().over(wNet))\n",
    "\n",
    "    return df_new\n",
    "\n",
    "'''\n",
    "    agg_lense(entity: str, p:str, input_dict: dict) -> spark dataframe\n",
    "        - calculates the total, positive, and negative volumes and percentages for each comparison entity within each lense \n",
    "        - ranks the comparison entities against their performance in each lense for positive, negative, and net sentiment\n",
    "\n",
    "    parameters: entity - the name of the column which has the entities you want to compare \n",
    "                    - inhereted from main_agg\n",
    "                p - the period in which you aggregating and calculating volumes, percentages, and rankings for the comparison entities \n",
    "                    - inherted from main_agg \n",
    "                    - takes values \"df_prev\" or \"df_rec\" \n",
    "                input_dict - the dictionary for \"Lense\" formatted as: \n",
    "                            input_dict = {'df_initial':spark_df, 'df_prev': spark_df, 'df_rec':spark_df, 'df_rankings': None}\n",
    "                            - inherted from main_agg\n",
    "\n",
    "    returns: a spark dataframe with the sentiment volumes, percentages, and rankings calculated at the lense level for all\n",
    "            comparison entities for a given time period\n",
    "'''\n",
    "\n",
    "\n",
    "def agg_lense(entity: str, p: str, input_dict: dict):\n",
    "    df = input_dict[p]\n",
    "    grouping = ['Category','Lense',entity]\n",
    "    \n",
    "    df_total = df.groupby(grouping).agg(ps.countDistinct('id').alias('total_vol'))\n",
    "  \n",
    "    df_pos = df.filter(ps.col('sentiment') == 1).groupby(grouping)\\\n",
    "              .agg(ps.countDistinct('id').alias('pos_vol')).withColumnRenamed('Category','Category_pos').withColumnRenamed(entity,entity+'_pos').withColumnRenamed('Lense','Lense_pos')\n",
    "\n",
    "    df_neg = df.filter(ps.col('sentiment') == -1).groupby(grouping)\\\n",
    "              .agg(ps.countDistinct('id').alias('neg_vol')).withColumnRenamed('Category','Category_neg').withColumnRenamed(entity,entity+'_neg').withColumnRenamed('Lense','Lense_neg')\n",
    "\n",
    "    df1 = df_total.join(df_pos, on = [df_total['Category'] == df_pos['Category_pos'],\n",
    "                                           df_total[entity] == df_pos[entity+'_pos'],\n",
    "                                           df_total['Lense'] == df_pos['Lense_pos']],\n",
    "                                      how = 'inner')\n",
    "\n",
    "    df_new = df1.join(df_neg, \n",
    "                         on = [df1['Category'] == df_neg['Category_neg'],\n",
    "                               df1[entity] == df_neg[entity+'_neg'],\n",
    "                               df1['Lense'] == df_neg['Lense_neg']],\n",
    "                                      how = 'inner')\\\n",
    "                        .drop('Category_pos')\\\n",
    "                        .drop(entity+'_pos')\\\n",
    "                        .drop('Lense_pos')\\\n",
    "                        .drop('Lense_neg')\\\n",
    "                        .drop('Category_neg')\\\n",
    "                        .drop(entity+'_neg')\n",
    "    \n",
    "    df_new = df_new.withColumn('percent_pos',  ps.round((ps.col('pos_vol')/ps.col('total_vol'))*100,2))\\\n",
    "            .withColumn('percent_neg', ps.round((ps.col('neg_vol')/ps.col('total_vol'))*100,2))\\\n",
    "            .withColumn('net_sent', ps.round((ps.col('percent_pos')-ps.col('percent_neg')),2))\n",
    "    \n",
    "    wPos = Window.partitionBy('Category', 'Lense').orderBy(ps.desc(\"percent_pos\"))\n",
    "    wNeg = Window.partitionBy('Category','Lense').orderBy(ps.desc(\"percent_neg\"))\n",
    "    wNet = Window.partitionBy('Category','Lense').orderBy(ps.desc(\"net_sent\"))\n",
    "    df_new = df_new.withColumn(\n",
    "        \"percent_pos_rank\", \n",
    "        ps.dense_rank().over(wPos)\n",
    "    ).withColumn(\n",
    "        \"percent_neg_rank\", \n",
    "        ps.dense_rank().over(wNeg)\n",
    "    ).withColumn(\n",
    "        \"net_sent_rank\",\n",
    "        ps.dense_rank().over(wNet))\n",
    "\n",
    "    return df_new\n",
    "\n",
    "'''\n",
    "    agg_sublense(entity: str, p:str, input_dict: dict) -> spark dataframe\n",
    "        - calculates the total, positive, and negative volumes and percentages for each comparison entity within each sublense \n",
    "        - ranks the comparison entities against their performance in each sublense for positive, negative, and net sentiment\n",
    "\n",
    "    parameters: entity - the name of the column which has the entities you want to compare \n",
    "                    - inhereted from main_agg\n",
    "                p - the period in which you aggregating and calculating volumes, percentages, and rankings for the comparison entities \n",
    "                    - inherted from main_agg \n",
    "                    - takes values \"df_prev\" or \"df_rec\" \n",
    "                input_dict - the dictionary for \"Sublense\" formatted as: \n",
    "                            input_dict = {'df_initial':spark_df, 'df_prev': spark_df, 'df_rec':spark_df, 'df_rankings': None}\n",
    "                            - inherted from main_agg\n",
    "\n",
    "    returns: a spark dataframe with the sentiment volumes, percentages, and rankings calculated at the lense level for all\n",
    "            comparison entities for a given time period\n",
    "'''\n",
    "\n",
    "def agg_sublense(entity: str, p: str, input_dict: dict):\n",
    "    df = input_dict[p]\n",
    "    grouping = ['Category','Lense','Sublense',entity]\n",
    "    df_total = df.groupby(grouping).agg(ps.countDistinct('id').alias('total_vol'))\n",
    "  \n",
    "    df_pos = df.filter(ps.col('sentiment') == 1).groupby(grouping)\\\n",
    "              .agg(ps.countDistinct('id').alias('pos_vol')).withColumnRenamed('Category','Category_pos').withColumnRenamed(entity,entity+'_pos').withColumnRenamed('Lense','Lense_pos')\\\n",
    "              .withColumnRenamed('Sublense','Sublense_pos')\n",
    "\n",
    "    df_neg = df.filter(ps.col('sentiment') == -1).groupby(grouping)\\\n",
    "              .agg(ps.countDistinct('id').alias('neg_vol')).withColumnRenamed('Category','Category_neg').withColumnRenamed(entity,entity+'_neg').withColumnRenamed('Lense','Lense_neg')\\\n",
    "              .withColumnRenamed('Sublense','Sublense_neg')\n",
    "\n",
    "    df1 = df_total.join(df_pos, on = [df_total['Category'] == df_pos['Category_pos'],\n",
    "                                           df_total[entity] == df_pos[entity+'_pos'],\n",
    "                                           df_total['Lense'] == df_pos['Lense_pos'],\n",
    "                                           df_total['Sublense'] == df_pos['Sublense_pos']\n",
    "                                           ],\n",
    "                                      how = 'inner')\n",
    "\n",
    "    df_new = df1.join(df_neg, \n",
    "                         on = [df1['Category'] == df_neg['Category_neg'],\n",
    "                               df1[entity] == df_neg[entity+'_neg'],\n",
    "                              df1['Lense'] == df_neg['Lense_neg'],\n",
    "                              df1['Sublense'] == df_neg['Sublense_neg']],\n",
    "                                      how = 'inner')\\\n",
    "                        .drop('Category_pos')\\\n",
    "                        .drop(entity+'_pos')\\\n",
    "                        .drop('Lense_pos')\\\n",
    "                        .drop('Lense_neg')\\\n",
    "                        .drop('Category_neg')\\\n",
    "                        .drop('Sublense_pos')\\\n",
    "                        .drop('Sublense_neg')\\\n",
    "                        .drop(entity+'_neg')\n",
    "    \n",
    "    df_new = df_new.withColumn('percent_pos',  ps.round((ps.col('pos_vol')/ps.col('total_vol'))*100,2))\\\n",
    "            .withColumn('percent_neg', ps.round((ps.col('neg_vol')/ps.col('total_vol'))*100,2))\\\n",
    "            .withColumn('net_sent', ps.round((ps.col('percent_pos')-ps.col('percent_neg')),2))\n",
    "    \n",
    "    wPos = Window.partitionBy('Category', 'Lense', 'Sublense').orderBy(ps.desc(\"percent_pos\"))\n",
    "    wNeg = Window.partitionBy('Category','Lense','Sublense').orderBy(ps.desc(\"percent_neg\"))\n",
    "    wNet = Window.partitionBy('Category','Lense','Sublense').orderBy(ps.desc(\"net_sent\"))\n",
    "    df_new = df_new.withColumn(\n",
    "        \"percent_pos_rank\", \n",
    "        ps.dense_rank().over(wPos)\n",
    "    ).withColumn(\n",
    "        \"percent_neg_rank\", \n",
    "        ps.dense_rank().over(wNeg)\n",
    "    ).withColumn(\n",
    "        \"net_sent_rank\",\n",
    "        ps.dense_rank().over(wNet))\n",
    "\n",
    "    return df_new\n",
    "\n",
    "# --------------------------------------------- ranking function utils ---------------------------------------------------------- # \n",
    "\n",
    "'''\n",
    "    rank_cat(entity: str, input_dict: dict) -> spark dataframe\n",
    "        - combines the net sentiment and the net sentiment rank for the baseline and the comparison periods into one dataframe and\n",
    "        calculates the net sentiment and the net sentiment rank change between the two periods at the category level\n",
    "\n",
    "    parameters: entity - the name of the column which has the entities you want to compare \n",
    "                    - inhereted from main_agg\n",
    "                input_dict - the dictionary for \"Category\" formatted as: \n",
    "                            input_dict = {'df_initial':spark_df, 'df_prev': spark_df, 'df_rec':spark_df, 'df_rankings': None}\n",
    "                            - inherted from main_rank\n",
    "    returns: a spark dataframe with the net sentiment, net sentiment rankings, net sentiment change, and net sentiment change for all\n",
    "            comparison entities at the \"Category\" level\n",
    "'''\n",
    "\n",
    "def rank_cat(entity: str, input_dict: dict):\n",
    "\n",
    "    df_prev = input_dict['df_prev']\n",
    "    df_rec = input_dict['df_rec']\n",
    "\n",
    "    df_prev_rank = df_prev.select('Category',entity,'net_sent','net_sent_rank')\\\n",
    "                    .withColumnRenamed('net_sent','net_sent_prev')\\\n",
    "                    .withColumnRenamed('net_sent_rank','net_sent_rank_prev')\n",
    "\n",
    "    df_rec_rank = df_rec.select('Category',entity,'net_sent','net_sent_rank')\\\n",
    "                .withColumnRenamed('Category','Category_rec')\\\n",
    "                .withColumnRenamed(entity,entity+'_rec')\\\n",
    "                .withColumnRenamed('net_sent','net_sent_rec')\\\n",
    "                .withColumnRenamed('net_sent_rank','net_sent_rank_rec')\n",
    "\n",
    "    df_rankings = df_prev_rank.join(df_rec_rank, on = [df_prev_rank['Category'] == df_rec_rank['Category_rec'],\n",
    "                                                    df_prev_rank[entity] == df_rec_rank[entity+'_rec']],\n",
    "                                how = 'inner')\\\n",
    "            .withColumn('net_sent_change', ps.round(ps.col('net_sent_rec')- ps.col('net_sent_prev'),2))\\\n",
    "            .withColumn('rank_change', ps.col('net_sent_rank_prev') - ps.col('net_sent_rank_rec'))\\\n",
    "            .drop('Category_rec')\\\n",
    "            .drop(entity+'_rec')\n",
    "\n",
    "    return df_rankings\n",
    "\n",
    "'''\n",
    "    rank_lense(entity: str, input_dict: dict) -> spark dataframe\n",
    "        - combines the net sentiment and the net sentiment rank for the baseline and the comparison periods into one dataframe and\n",
    "        calculates the net sentiment and the net sentiment rank change between the two periods at the lense level\n",
    "\n",
    "    parameters: entity - the name of the column which has the entities you want to compare \n",
    "                    - inhereted from main_agg\n",
    "                input_dict - the dictionary for \"Lense\" formatted as: \n",
    "                            input_dict = {'df_initial':spark_df, 'df_prev': spark_df, 'df_rec':spark_df, 'df_rankings': None}\n",
    "                            - inherted from main_rank\n",
    "    returns: a spark dataframe with the net sentiment, net sentiment rankings, net sentiment change, and net sentiment change for all\n",
    "            comparison entities at the \"Lense\" level\n",
    "'''\n",
    "\n",
    "def rank_lense(entity: str, input_dict: dict):\n",
    "\n",
    "    df_prev = input_dict['df_prev']\n",
    "    df_rec = input_dict['df_rec']\n",
    "\n",
    "    df_prev_rank = df_prev.select('Category','Lense',entity,'net_sent','net_sent_rank')\\\n",
    "                    .withColumnRenamed('net_sent','net_sent_prev')\\\n",
    "                    .withColumnRenamed('net_sent_rank','net_sent_rank_prev')\n",
    "\n",
    "    df_rec_rank = df_rec.select('Category','Lense',entity,'net_sent','net_sent_rank')\\\n",
    "                .withColumnRenamed('Category','Category_rec')\\\n",
    "                .withColumnRenamed('Lense','Lense_rec')\\\n",
    "                .withColumnRenamed(entity,entity+'_rec')\\\n",
    "                .withColumnRenamed('net_sent','net_sent_rec')\\\n",
    "                .withColumnRenamed('net_sent_rank','net_sent_rank_rec')\n",
    "\n",
    "    df_rankings = df_prev_rank.join(df_rec_rank, on = [df_prev_rank['Category'] == df_rec_rank['Category_rec'],\n",
    "                                                    df_prev_rank['Lense'] == df_rec_rank['Lense_rec'],\n",
    "                                                    df_prev_rank[entity] == df_rec_rank[entity+'_rec']],\n",
    "                                how = 'inner')\\\n",
    "            .withColumn('net_sent_change', ps.round(ps.col('net_sent_rec')- ps.col('net_sent_prev'),2))\\\n",
    "            .withColumn('rank_change', ps.col('net_sent_rank_prev') - ps.col('net_sent_rank_rec'))\\\n",
    "            .drop('Category_rec')\\\n",
    "            .drop('Lense_rec')\\\n",
    "            .drop(entity+'_rec')\n",
    "\n",
    "    return df_rankings\n",
    "\n",
    "'''\n",
    "    rank_sublense(entity: str, input_dict: dict) -> spark dataframe\n",
    "        - combines the net sentiment and the net sentiment rank for the baseline and the comparison periods into one dataframe and\n",
    "        calculates the net sentiment and the net sentiment rank change between the two periods at the sublense level\n",
    "\n",
    "    parameters: entity - the name of the column which has the entities you want to compare \n",
    "                    - inhereted from main_agg\n",
    "                input_dict - the dictionary for \"Sublense\" formatted as: \n",
    "                            input_dict = {'df_initial':spark_df, 'df_prev': spark_df, 'df_rec':spark_df, 'df_rankings': None}\n",
    "                            - inherted from main_rank\n",
    "    returns: a spark dataframe with the net sentiment, net sentiment rankings, net sentiment change, and net sentiment change for all\n",
    "            comparison entities at the \"Sublense\" level\n",
    "'''\n",
    "\n",
    "def rank_sublense(entity:str, input_dict: dict):\n",
    "\n",
    "    df_prev = input_dict['df_prev']\n",
    "    df_rec = input_dict['df_rec']\n",
    "\n",
    "    df_prev_rank = df_prev.select('Category','Lense','Sublense',entity,'net_sent','net_sent_rank')\\\n",
    "                    .withColumnRenamed(entity,entity)\\\n",
    "                    .withColumnRenamed('net_sent','net_sent_prev')\\\n",
    "                    .withColumnRenamed('net_sent_rank','net_sent_rank_prev')\n",
    "\n",
    "    df_rec_rank = df_rec.select('Category','Lense','Sublense',entity,'net_sent','net_sent_rank')\\\n",
    "                .withColumnRenamed('Category','Category_rec')\\\n",
    "                .withColumnRenamed('Lense','Lense_rec')\\\n",
    "                .withColumnRenamed('Sublense','Sublense_rec')\\\n",
    "                .withColumnRenamed(entity,entity+'_rec')\\\n",
    "                .withColumnRenamed('net_sent','net_sent_rec')\\\n",
    "                .withColumnRenamed('net_sent_rank','net_sent_rank_rec')\n",
    "\n",
    "    df_rankings = df_prev_rank.join(df_rec_rank, on = [df_prev_rank['Category'] == df_rec_rank['Category_rec'],\n",
    "                                                    df_prev_rank['Lense'] == df_rec_rank['Lense_rec'],\n",
    "                                                    df_prev_rank['Sublense'] == df_rec_rank['Sublense_rec'],\n",
    "                                                    df_prev_rank[entity] == df_rec_rank[entity+'_rec']],\n",
    "                                how = 'inner')\\\n",
    "            .withColumn('net_sent_change', ps.round(ps.col('net_sent_rec')- ps.col('net_sent_prev'),2))\\\n",
    "            .withColumn('rank_change', ps.col('net_sent_rank_prev') - ps.col('net_sent_rank_rec'))\\\n",
    "            .drop('Category_rec')\\\n",
    "            .drop('Lense_rec')\\\n",
    "            .drop('Sublense_rec')\\\n",
    "            .drop(entity+'_rec')\n",
    "\n",
    "    return df_rankings\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#from utils import *\n",
    "\n",
    "''' \n",
    "    data_prep(spark_df: spark dataframe, level_list: list, entity_lense: str, entity: str, excluded_category: str,\n",
    "            baseline_dates: tuple, comparison_dates: tuple) -> dictionary\n",
    "\n",
    "            - takes in parameters from the notebook widgets to filter the input spark dataframe\n",
    "            - divides the original dataframe up into the specified levels (provided by level_list) and for each level \n",
    "            creates two dataframes (df_prev and df_rec) based on dates provided in baseline_dates and comparison_dates\n",
    "            \n",
    "    parameters: spark_df - raw spark dataframe corresponding to given sentiment curation pipeline output ID\n",
    "\n",
    "                level_list - list of level(s) (Category, Lense, Sublense) that the sentiment rankings will be \n",
    "                calculated for given by level widget selection\n",
    "                    - inherted from main\n",
    "\n",
    "                entity_lense - name of lense in taxonomy that contains entities which you want to compare against each other\n",
    "                across the rest of the taxonomy (i.e you want to compare airline performance across customer satisfaction taxonomy, \n",
    "                so you have a lense for airlines where the sublenses are the airlines which you want to compare against each other) \n",
    "                - given by Comp Ent Lense widget\n",
    "                    - inherted from main\n",
    "\n",
    "                entity - the name of your entity (i.e if you are comparing airlines your entity name shoud be \"airline\", if you are \n",
    "                comparing food brands, your entity name should be \"food brand) - given by Comp Ent Name widget\n",
    "                    - inherted from main\n",
    "\n",
    "                - excluded_category - if you have a category that has extraneous factors that you do not care to compare entities against\n",
    "                specify the name of the category here \n",
    "                    - inherted from main\n",
    "\n",
    "                - baseline_dates - a tuple in YYYY-MM-DD format specifying the start and end dates for the baseline period\n",
    "                    - inherted from main\n",
    "\n",
    "                - comparison_dates - a tuple in YYYY-MM-DD format specifying the start and end dates for the comparison period\n",
    "                    - inherted from main\n",
    "\n",
    "    returns: a nested dictionary of format:\n",
    "            {'Category': {'df_initial': spark_df, 'df_prev': spark_df, 'df_rec': spark_df, 'df_rankings': none},\n",
    "            'Lense': {'df_initial': spark_df, 'df_prev': spark_df, 'df_rec': spark_df, 'df_rankings': none},\n",
    "            'Sublense': {'df_initial': spark_df, 'df_prev': spark_df, 'df_rec': spark_df, 'df_rankings': none}}\n",
    "\n",
    "            df_initial is the input spark df filtered and reformatted version according to specified comparison entity lense\n",
    "            df_prev is df_initial filtered by the baseline_dates \n",
    "            df_rec is df_initial filtered by the comparison_dates \n",
    "'''\n",
    "\n",
    "def data_prep(spark_df, level_list, entity_lense, entity, excluded_category,baseline_dates,comparison_dates):\n",
    "    \n",
    "    input_dict = {}\n",
    "\n",
    "    cols = [ps.col(\"id\"), ps.col(\"date\"), ps.col(\"taxonomies\"), ps.col(\"message\"), ps.col(\"sentiment\")]\n",
    "\n",
    "    entity_df = spark_df.select(\n",
    "                cols + [ps.explode(ps.col(\"taxonomies.taxonomyPath\")).alias(\"taxonomyPath\")])\\\n",
    "            .where(\n",
    "                ps.element_at(ps.col(\"taxonomyPath\"), 2) == ps.lit(entity_lense))\\\n",
    "            .select(\n",
    "                cols + [ps.element_at(ps.col(\"taxonomyPath\"), 3).alias(entity)]\n",
    "            )\n",
    "\n",
    "    for c in level_list:\n",
    "\n",
    "        inner_dict = {'df_initial': None, 'df_prev': None, 'df_rec': None, 'df_rankings': None}\n",
    "        input_dict[c] = inner_dict\n",
    "\n",
    "        entity_df = entity_df.select(\n",
    "                cols + [entity] + [ps.explode(ps.col(\"taxonomies.taxonomyPath\")).alias(\"taxonomyPath\")])\\\n",
    "            .select(\n",
    "                cols + [entity] + [ps.element_at(ps.col(\"taxonomyPath\"), 1).alias(\"Category\")] + \n",
    "                [ps.element_at(ps.col(\"taxonomyPath\"), 2).alias(\"Lense\")] + [ps.element_at(ps.col(\"taxonomyPath\"), 3).alias(\"Sublense\")]\n",
    "            )\n",
    "\n",
    "        if len(excluded_category) > 0:\n",
    "            entity_df = entity_df.filter(ps.col('Category') != excluded_category)\n",
    "        else:\n",
    "            entity_df = entity_df\n",
    "        \n",
    "        entity_df = entity_df.withColumn(\"year\", ps.year(ps.col(\"date\"))).withColumn(\"month\", ps.month(ps.col(\"date\"))).withColumn('day', ps.dayofmonth(ps.col(\"date\")))\\\n",
    "                    .withColumn(\"date_str\",  ps.concat_ws(\"_\", (ps.col(\"year\")), ps.format_string('%02d', (ps.col(\"month\")))))\\\n",
    "                    .withColumn('dt',ps.to_timestamp(ps.col('date_str'), 'yyyy_MM'))\n",
    "                    \n",
    "\n",
    "        input_dict[c]['df_initial'] = entity_df\\\n",
    "                                    .drop('date')\\\n",
    "                                    .drop('year')\\\n",
    "                                    .drop('month')\\\n",
    "                                    .drop('taxonomies')\\\n",
    "                                    .drop('message')\n",
    "        input_dict[c]['df_prev'] = entity_df.where(ps.col('dt').between(*baseline_dates))\\\n",
    "                                .drop('date')\\\n",
    "                                .drop('year')\\\n",
    "                                .drop('month')\\\n",
    "                                .drop('taxonomies')\\\n",
    "                                .drop('message')\n",
    "        input_dict[c]['df_rec'] = entity_df.where(ps.col('dt').between(*comparison_dates))\\\n",
    "                                .drop('date')\\\n",
    "                                .drop('year')\\\n",
    "                                .drop('month')\\\n",
    "                                .drop('taxonomies')\\\n",
    "                                .drop('message')\n",
    "    \n",
    "    return input_dict \n",
    "                \n",
    "\n",
    "''' \n",
    "    main_agg(entity: str, level_list: list, period_list: list, input_dict: dict) -> dictionary\n",
    "        - calculates the sentiment, volumes, and ranks for levels specified in level_list across the baseline and comparison time periods\n",
    "    \n",
    "    parameters: entity - name of comparison entity \n",
    "                    - inhereted from main \n",
    "                level_list - list of levels for which SentRank should be computed \n",
    "                    -inherted from main\n",
    "                period_list - list of periods for which SentRank should be computed\n",
    "                    -inhereted from main\n",
    "                input_dict - dictionary formatted as: \n",
    "                    - {'Category': {'df_initial': spark_df, 'df_prev': spark_df, 'df_rec': spark_df, 'df_rankings': none},\n",
    "                        'Lense': {'df_initial': spark_df, 'df_prev': spark_df, 'df_rec': spark_df, 'df_rankings': none},\n",
    "                        'Sublense': {'df_initial': spark_df, 'df_prev': spark_df, 'df_rec': spark_df, 'df_rankings': none}}\n",
    "                    - inherted from output of data_prep function in main function call\n",
    "\n",
    "'''\n",
    "\n",
    "def main_agg(entity: str, level_list: list, period_list: list, input_dict: dict):\n",
    "    result_dict = {}\n",
    "    for l in level_list:\n",
    "        inner_dict = {}\n",
    "        for p in period_list:\n",
    "            inner_dict[p] = None\n",
    "        result_dict[l] = inner_dict\n",
    "\n",
    "    for l in level_list:\n",
    "        for p in period_list:\n",
    "            if l == 'Category':\n",
    "                result_dict[l][p] = agg_cat(entity, p, input_dict[l])\n",
    "            elif l == 'Lense':\n",
    "                result_dict[l][p] = agg_lense(entity, p, input_dict[l])\n",
    "            else:\n",
    "                result_dict[l][p] = agg_sublense(entity, p, input_dict[l])\n",
    "    return result_dict\n",
    "\n",
    "''' \n",
    "    main_rank(entity: str, level_list: list, input_dict: dict) -> dictionary\n",
    "        - aggregates the net sentiment, net sentiment rank across the baseline and comparison time periods \n",
    "            and calculates net sentiment/net sentiment rank change between periods for levels specified in level_list \n",
    "    \n",
    "    parameters: entity - name of comparison entity \n",
    "                    - inhereted from main \n",
    "                level_list - list of levels for which SentRank should be computed \n",
    "                    -inherted from main\n",
    "                input_dict - dictionary formatted as: \n",
    "                    - {'Category': {'df_initial': spark_df, 'df_prev': spark_df, 'df_rec': spark_df, 'df_rankings': none},\n",
    "                        'Lense': {'df_initial': spark_df, 'df_prev': spark_df, 'df_rec': spark_df, 'df_rankings': none},\n",
    "                        'Sublense': {'df_initial': spark_df, 'df_prev': spark_df, 'df_rec': spark_df, 'df_rankings': none}}\n",
    "                    - inherted from output of agg_main function in main function call\n",
    "\n",
    "'''\n",
    "\n",
    "def main_rank(entity: str, level_list: list, input_dict: dict):\n",
    "\n",
    "    for l in level_list:\n",
    "        if l == 'Category':\n",
    "            input_dict[l]['df_rankings'] = rank_cat(entity, input_dict[l])\n",
    "        elif l == 'Lense':\n",
    "            input_dict[l]['df_rankings'] = rank_lense(entity, input_dict[l])\n",
    "        else:\n",
    "            input_dict[l]['df_rankings'] = rank_sublense(entity, input_dict[l])\n",
    "    return input_dict\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#from sentRank_prep_agg_rank import *\n",
    "\n",
    "''' \n",
    "    main(spark_df: spark dataframe, level_list: list, entity_lense: str, entity: str, excluded_category: str,\n",
    "        baseline_dates: tuple ,comparison_dates: tuple ,period_list: list)\n",
    "\n",
    "    parameters: spark_df - raw spark dataframe corresponding to given sentiment curation pipeline output ID\n",
    "                    - given by Sent Curation Output ID widget\n",
    "\n",
    "                level_list - list of level(s) (Category, Lense, Sublense) that the sentiment rankings will be \n",
    "                calculated for \n",
    "                    - given by level widget selection\n",
    "\n",
    "                entity_lense - name of lense in taxonomy that contains entities which you want to compare against each other\n",
    "                across the rest of the taxonomy (i.e you want to compare airline performance across customer satisfaction taxonomy, \n",
    "                so you have a lense for airlines where the sublenses are the airlines which you want to compare against each other) \n",
    "                    - given by Comp Ent Lense widget\n",
    "\n",
    "                entity - the name of your entity (i.e if you are comparing airlines your entity name shoud be \"airline\", if you are \n",
    "                comparing food brands, your entity name should be \"food brand) \n",
    "                    - given by Comp Ent Name widget\n",
    "\n",
    "                - excluded_category - if you have a category that has extraneous factors that you do not care to compare entities against\n",
    "                specify the name of the category here \n",
    "                    - given by Excluded Cat widget\n",
    "\n",
    "                - baseline_dates - a tuple in YYYY-MM-DD format specifying the start and end dates for the baseline period\n",
    "                    - given by Prev Period End Date and Prev Period Start Date widgets\n",
    "\n",
    "                - comparison_dates - a tuple in YYYY-MM-DD format specifying the start and end dates for the comparison period\n",
    "                    - given by Recent Period End Date and Recent Period Start Date widgets\n",
    "                \n",
    "                - period_list - ['df_prev','df_rec']\n",
    "\n",
    "    returns: nested dictionry formatted as: \n",
    "                - {'Category': {'df_initial': spark_df, 'df_prev': spark_df, 'df_rec': spark_df, 'df_rankings': none},\n",
    "                    'Lense': {'df_initial': spark_df, 'df_prev': spark_df, 'df_rec': spark_df, 'df_rankings': none},\n",
    "                    'Sublense': {'df_initial': spark_df, 'df_prev': spark_df, 'df_rec': spark_df, 'df_rankings': none}}\n",
    "\n",
    "                - each nested dictionary within in each level is the result of main_agg and main_rank\n",
    "'''\n",
    "\n",
    "def main(spark_df, level_list, entity_lense, entity, excluded_category,baseline_dates,comparison_dates,period_list):\n",
    "    \n",
    "    #prep data from raw sentiment curation pipeline output id --> input_dict\n",
    "    input_dict = data_prep(spark_df, level_list, entity_lense, entity, excluded_category,baseline_dates,comparison_dates)\n",
    "\n",
    "    #calculate sentiment volumes, percentages, and rakings for each level in in each period\n",
    "    temp_dict = main_agg(entity, level_list, period_list, input_dict)\n",
    "\n",
    "    #calculate net sentiment rankings and changes in net sentiment and rakings between periods\n",
    "    result_dict = main_rank(entity, level_list, temp_dict)\n",
    "\n",
    "    return input_dict,result_dict\n",
    "  \n",
    "input_dict,result_dict = main(df, levels, comp_ent_lense, comp_ent_name, excluded_cat,p_dates,r_dates,['df_prev','df_rec'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SentRank Results for Basline Time Period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(result_dict['Category']['df_prev'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(result_dict['Lense']['df_prev'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(result_dict['Sublense']['df_prev'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sent Results for Comparison Time Period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(result_dict['Category']['df_rec'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(result_dict['Lense']['df_rec'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(result_dict['Sublense']['df_rec'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Rankings Results by Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(result_dict['Category']['df_rankings'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(result_dict['Lense']['df_rankings'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(result_dict['Sublense']['df_rankings'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write to Excel in Specified Working Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#helper funcs to write to excel\n",
    "def check_copyfile(copy_path: str, writing_path: str, databricks_write: bool):\n",
    "    if databricks_write:\n",
    "        copyfile(copy_path, writing_path)\n",
    "\n",
    "def writing_check(writing_path: str, databricks_write: bool, writing_name: str) -> str:\n",
    "    if databricks_write:\n",
    "        copy_path = \"/tmp/\" + writing_name + \".xlsx\"\n",
    "    else:\n",
    "        copy_path = writing_path\n",
    "    return copy_path\n",
    "\n",
    "def df_to_file(result_dict: dict, writing_path: str, databricks_write: bool = True) -> str:\n",
    "  copied_path = writing_check(writing_path, databricks_write, \"deduplication_result\")\n",
    "  with pd.ExcelWriter(copied_path) as writer:\n",
    "      for key in result_dict.keys():\n",
    "        for t in result_dict[key].keys():\n",
    "          result_dict[key][t].to_excel(writer, sheet_name=key+\"_\"+t)\n",
    "  check_copyfile(copied_path, writing_path, databricks_write)\n",
    "  return writing_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">/databricks/spark/python/pyspark/sql/types.py:1636: DeprecationWarning: Using or importing the ABCs from &#39;collections&#39; instead of from &#39;collections.abc&#39; is deprecated, and in 3.8 it will stop working\n",
       "  return pa.schema(fields)\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if to_excel == 'Yes':\n",
    "  pandas_dict = {}\n",
    "  for key in result_dict.keys():\n",
    "    inner_dict = {}\n",
    "    for t in result_dict[key].keys():\n",
    "      if t != 'df_initial':\n",
    "        inner_dict[t] = result_dict[key][t].toPandas()\n",
    "      else:\n",
    "        continue\n",
    "    pandas_dict[key] = inner_dict\n",
    "    \n",
    "  #write to excel in working dir\n",
    "  out_file_prefix =  output_path + \"/\" + output_filename\n",
    "  writing_path = \"/dbfs/mnt/\" + out_file_prefix + \".xlsx\"\n",
    "  df_to_file(pandas_dict,writing_path, True)\n",
    "\n",
    "  #write standardized df to pickle \n",
    "  with open(\"/dbfs/mnt\"+output_path+\"/\"+output_filename+\".pkl\", \"wb\") as file:\n",
    "    pickle.dump(pandas_dict, file)\n",
    "    \n",
    "else:\n",
    "  print(\"Results NOT SAVED - if you wanted to save your results in an excel file please select 'Yes' from widget 12 and re-run commands 5 and 23\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#have to manually write output file to bsinsights bucket to be able to download\n",
    "writing_path = \"/dbfs/mnt/bsinsights/Raim/covid_rankings_May19\" + \".xlsx\"\n",
    "df_to_file(pandas_dict,writing_path, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "name": "05-5 Sentiment Ranking Pipeline MASTER",
  "notebookId": 880046
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
